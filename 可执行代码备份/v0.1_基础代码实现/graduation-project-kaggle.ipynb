{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7962084,"sourceType":"datasetVersion","datasetId":4683889}],"dockerImageVersionId":30674,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2024-03-28T08:48:05.606728Z","iopub.execute_input":"2024-03-28T08:48:05.607525Z","iopub.status.idle":"2024-03-28T08:48:09.186953Z","shell.execute_reply.started":"2024-03-28T08:48:05.607487Z","shell.execute_reply":"2024-03-28T08:48:09.185614Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"cpu\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\n\n# 打开指定目录\nos.chdir(\"/kaggle/working/\")\n# 创建新文件夹\nos.makedirs(\"log\")\nos.makedirs(\"weights\")","metadata":{"execution":{"iopub.status.busy":"2024-03-28T08:51:50.210532Z","iopub.execute_input":"2024-03-28T08:51:50.210907Z","iopub.status.idle":"2024-03-28T08:51:50.223568Z","shell.execute_reply.started":"2024-03-28T08:51:50.210866Z","shell.execute_reply":"2024-03-28T08:51:50.222674Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# import logging\n# import sys\n# sys.path.append('/kaggle/working')\n\n\n# # 创建一个logger  \n# logger = logging.getLogger(\"/log/vgg11_training\")  \n# logger.setLevel(logging.INFO)  \n  \n# # 创建一个handler，用于写入日志文件  \n# log_file = \"vgg11_training.log\"  \n# fh = logging.FileHandler(log_file)  \n# fh.setLevel(logging.INFO)  \n  \n# # 再创建一个handler，用于输出到控制台  \n# ch = logging.StreamHandler()  \n# ch.setLevel(logging.INFO)  \n  \n# # 定义handler的输出格式  \n# formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')  \n# fh.setFormatter(formatter)  \n# ch.setFormatter(formatter)  \n  \n# # 给logger添加handler  \n# logger.addHandler(fh)  \n# logger.addHandler(ch)","metadata":{"execution":{"iopub.status.busy":"2024-03-28T08:52:01.566467Z","iopub.execute_input":"2024-03-28T08:52:01.566850Z","iopub.status.idle":"2024-03-28T08:52:01.574633Z","shell.execute_reply.started":"2024-03-28T08:52:01.566819Z","shell.execute_reply":"2024-03-28T08:52:01.573643Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# 表示我们当前的项目路径是在input下的\nimport sys\nsys.path.append('../input/graduation-project')\n# print(sys.path)\n\nfrom math import ceil\n\nimport torch\nfrom torch.utils.data import DataLoader, random_split\nimport torchvision\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n# 包的路径改成当前路径（加上graduation-project.program.）\nfrom program.utils.myloss import MyLoss\nfrom program.dataset.read_yolo_dataset import ReadYOLO\nfrom program.Augmentation.data_augment import DataAugment\n\nimport logging\nimport time\n\n# import argparse  # 可以直接在命令行中向程序传入参数并让程序运行\n# # 在命令行运行时可以加以下参数进行修改\n# parser = argparse.ArgumentParser(description='Training')\n# parser.add_argument('--model', default=\"vgg11\", help='model')  # 选择模型\n# parser.add_argument('--dateset_address', default=\"/kaggle/input/graduation-project/program/dataset\", help='dateset_address')  # 数据集地址\n# parser.add_argument('--train_rate', default=0.8, type=float, help='train_rate')  # 训练集切分比例\n# parser.add_argument('--lr', default=0.001, type=float, help='learning rate of model')  # 学习率\n# parser.add_argument('--momentum', default=0.9, type=float, help='momentum')  # 动量\n# parser.add_argument('--batch_size', default=32, type=int, help='batch_size')\n# parser.add_argument('--epochs', default=20, type=int, help='epochs')\n# parser.add_argument('--weight_decay', default=5e-4, type=float, help='Weight decay for SGD')  # SGD的权重衰减\n# args = parser.parse_args()\n\nmodel=\"vgg11\"\ndateset_address=\"/kaggle/input/graduation-project/program/dataset\"\ntrain_rate=0.8\nlr=0.001\nmomentum=0.9\nbatch_size=64\nepochs=20\nweight_decay=5e-4\n\n# 创建一个logger  \nlogger = logging.getLogger(f\"/log/{model}_training\")  \nlogger.setLevel(logging.INFO)  \n# 创建一个handler，用于写入日志文件  \nlog_file = f\"{model}_training.log\"  \nfh = logging.FileHandler(log_file)  \nfh.setLevel(logging.INFO)  \n# 再创建一个handler，用于输出到控制台  \nch = logging.StreamHandler()  \nch.setLevel(logging.INFO)  \n# 定义handler的输出格式  \nformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')  \nfh.setFormatter(formatter)  \nch.setFormatter(formatter)  \n# 给logger添加handler  \nlogger.addHandler(fh)  \nlogger.addHandler(ch)\n\n# 创建全局device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# 读取数据集\nsize = (224, 224)\ndata_augment = DataAugment(size=size)  # 数据增强实例化\ndataset = ReadYOLO(dateset_address=dateset_address, phase='train', trans=data_augment, device=device)  # 读取数据集实例化\npicture_num = len(dataset)  # 获取图片总数\n\nkwargs = {\"num_classes\": 2}\n\n# 模型实例化\nnet = torchvision.models.vgg16()\n# alexnet vgg11/13/16/19(_bn) googlenet resnet18/34/50/101/152 densenet121/161/169/201 convnext_tiny/small/base/large\nmodel_str = f\"net = torchvision.models.{model}(**{kwargs})\"\nexec(model_str)\nnet = net.to(device=device)\n\n# 迭代器和损失函数优化器实例化\noptimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\nloss = MyLoss()  # 等价于loss = nn.CrossEntropyLoss()，loss(网络获取的图片)\n\n\n# 创建图片数据迭代器\ndef colle(batch):\n    # batch内多个元组形成一个元组，*解压出多个元组，zip每个对应位置缝合（相同索引）\n    imgs, targets = list(zip(*batch))\n    # 图片合并标签不合并可以加速训练（此处都合并了）\n    imgs = torch.cat(imgs, dim=0)  # cat(inputs, dim=?)在给定维度上对输入的张量序列seq 进行连接操作。\n    targets = torch.cat(targets, dim=0)  # tensor([1,]),tensor([0,])……（shape为[1,]）合并为tensor([1,0])\n    return imgs, targets\n\n\n# 若实现了__len__和__getitem__，DataLoader会自动实现数据集的分批，shuffle打乱顺序，drop_last删除最后不完整的批次，collate_fn如何取样本\ndataload = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=False, collate_fn=colle)\n\n\ndef train():\n#     log_file = open(f'/kaggle/working/log/{model}_training_log.txt', 'w')\n\n    global net\n#     epochs = epochs  # 设置epoch\n\n    for epoch in range(epochs):\n        start_epoch = time.time()  # epoch开始计时\n        # 切分训练集和测试集\n        trainset, testset = random_split(dataset, lengths=[train_rate, 1 - train_rate],\n                                         generator=torch.Generator().manual_seed(0))\n        trainLoader = DataLoader(trainset, batch_size=batch_size, shuffle=True, drop_last=False, collate_fn=colle)\n        testLoader = DataLoader(testset, batch_size=batch_size, shuffle=True, drop_last=False, collate_fn=colle)\n\n        # 开启训练模式（BatchNorm和DropOut被使用，net.eval()推理模式会屏蔽这些模块）\n        logger.info(f\"Train_epoch:{epoch + 1}/{epochs}\\n\")\n\n        net.train()\n        Loss = 0\n        total_loss = 0\n        batch_count = 0  # 对batch计数\n        batch_counts = ceil(len(dataset) * train_rate / batch_size)\n        for batch, (imgs, targets) in enumerate(trainLoader):\n            start_batch = time.time()  # batch开始计时\n            batch_count += 1\n            # 训练主体\n            # alexnet, vgg, resnet\n            pred = net(imgs)  # imgs大小(batch_size,3,224,224)\n            Loss = loss(pred, targets)\n            total_loss += Loss\n            optimizer.zero_grad()  # 优化器梯度归零\n            Loss.backward()  # 反向传播计算梯度\n            optimizer.step()  # 更新参数\n\n            # # GoogLeNet\n            # pred, aux2, aux1 = net(imgs)\n            # main_loss = loss(pred, targets)\n            # # 计算辅助输出的损失\n            # aux2_loss = loss(aux2.view(aux2.size(0), -1), targets)\n            # aux1_loss = loss(aux1.view(aux1.size(0), -1), targets)\n            # Loss = main_loss + 0.3 * aux2_loss + 0.3 * aux1_loss\n            # total_loss += Loss\n            # optimizer.zero_grad()\n            # Loss.backward()\n            # optimizer.step()\n\n            # if Loss <= 1e-3:\n            #     # loss达到理想值，提前终止\n            #     torch.save(net.state_dict(), \"./weights/A_Early_stop_epoch{}_params.pth\".format(epoch + 1))\n            #     return print(\"训练结束\")\n\n            # 打印参数\n            batch_time = time.time() - start_batch  # 训练一个epoch的时间\n            logger.info(f\"batch:{batch_count}/{batch_counts}, \"\n                           f\"loss:{float(Loss):.4f}, \"\n                           f\"batch_time:{batch_time:.4f}\\n\")\n\n        logger.info(f'Epoch {epoch + 1}/{epochs}, total_loss: {float(total_loss):.4f}\\n')\n\n        torch.save(net.state_dict(), f\"/kaggle/working/weights/{model}_epoch{epoch + 1}_params.pth\")  # 每个epoch保存一次参数\n\n        logger.info(f\"Test_epoch:{epoch + 1}/{epochs}\\n\")\n\n        net.eval()\n        preds = []  # 从模型获得预测结果\n        true_labels = []  # 真实标签\n        test_start_time = time.time()\n        for batch, (imgs, targets) in enumerate(testLoader):\n            # 测试主体\n            pred = net(imgs).detach().cpu().numpy()  # 预测结果\n            target = targets.ravel().cpu().numpy()\n            preds.append(pred)\n            true_labels.append(target)\n        tensor_preds = [torch.from_numpy(pred) for pred in preds]\n        preds = torch.cat(tensor_preds, dim=0)\n        tensor_true_labels = [torch.from_numpy(label) for label in true_labels]\n        true_labels = torch.cat(tensor_true_labels, dim=0)\n\n        accuracy = accuracy_score(true_labels, preds.argmax(dim=1))\n        precision = precision_score(true_labels, preds.argmax(dim=1), average='macro')\n        recall = recall_score(true_labels, preds.argmax(dim=1), average='macro')\n        f1 = f1_score(true_labels, preds.argmax(dim=1), average='macro')\n\n        test_time = time.time() - test_start_time\n        logger.info(f'Test Accuracy: {accuracy:.4f}, '\n                       f'Test Precision: {precision:.4f}, '\n                       f'Test Recall: {recall:.4f}, '\n                       f'Test F1: {f1:.4f} '\n                       f'Test time: {test_time}\\n')\n\n\n        # 打印参数\n        total_epoch_time = time.time() - start_epoch  # 训练一个epoch的时间\n        epoch_hour = int(total_epoch_time / 60 // 60)\n        epoch_minute = int(total_epoch_time // 60 - epoch_hour * 60)\n        epoch_second = int(total_epoch_time - epoch_hour * 60 * 60 - epoch_minute * 60)\n        logger.info(f\"epoch:{epoch + 1}/{epochs}, \"\n                       f\"total_time:{epoch_hour}:{epoch_minute}:{epoch_second}\\n\")\n         \n\n    print(\"训练结束\")\n\n    log_file.close()\n\n\nif __name__ == '__main__':\n    \n    train()\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-28T08:52:07.103563Z","iopub.execute_input":"2024-03-28T08:52:07.103908Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"2024-03-28 08:52:21,075 - vgg11_training - INFO - Train_epoch:1/20\n\n2024-03-28 08:52:26,276 - vgg11_training - INFO - batch:1/160, loss:0.7001, batch_time:1.1101\n\n2024-03-28 08:52:30,563 - vgg11_training - INFO - batch:2/160, loss:0.6993, batch_time:0.2651\n\n2024-03-28 08:52:34,412 - vgg11_training - INFO - batch:3/160, loss:0.6953, batch_time:0.0865\n\n2024-03-28 08:52:38,459 - vgg11_training - INFO - batch:4/160, loss:0.6976, batch_time:0.0867\n\n2024-03-28 08:52:42,225 - vgg11_training - INFO - batch:5/160, loss:0.6957, batch_time:0.0863\n\n2024-03-28 08:52:46,198 - vgg11_training - INFO - batch:6/160, loss:0.6914, batch_time:0.0863\n\n2024-03-28 08:52:50,197 - vgg11_training - INFO - batch:7/160, loss:0.6846, batch_time:0.0866\n\n2024-03-28 08:52:53,982 - vgg11_training - INFO - batch:8/160, loss:0.6856, batch_time:0.0861\n\n2024-03-28 08:52:58,128 - vgg11_training - INFO - batch:9/160, loss:0.6829, batch_time:0.0862\n\n2024-03-28 08:53:02,236 - vgg11_training - INFO - batch:10/160, loss:0.6798, batch_time:0.0862\n\n2024-03-28 08:53:06,278 - vgg11_training - INFO - batch:11/160, loss:0.6652, batch_time:0.0867\n\n2024-03-28 08:53:10,355 - vgg11_training - INFO - batch:12/160, loss:0.6642, batch_time:0.0863\n\n2024-03-28 08:53:14,214 - vgg11_training - INFO - batch:13/160, loss:0.6643, batch_time:0.0868\n\n2024-03-28 08:53:18,416 - vgg11_training - INFO - batch:14/160, loss:0.6660, batch_time:0.0924\n\n2024-03-28 08:53:22,714 - vgg11_training - INFO - batch:15/160, loss:0.6552, batch_time:0.0863\n\n2024-03-28 08:53:26,742 - vgg11_training - INFO - batch:16/160, loss:0.6522, batch_time:0.0859\n\n2024-03-28 08:53:30,780 - vgg11_training - INFO - batch:17/160, loss:0.6333, batch_time:0.0869\n\n2024-03-28 08:53:34,676 - vgg11_training - INFO - batch:18/160, loss:0.6329, batch_time:0.0862\n\n2024-03-28 08:53:38,385 - vgg11_training - INFO - batch:19/160, loss:0.6493, batch_time:0.0878\n\n2024-03-28 08:53:42,365 - vgg11_training - INFO - batch:20/160, loss:0.6464, batch_time:0.0863\n\n2024-03-28 08:53:46,758 - vgg11_training - INFO - batch:21/160, loss:0.6216, batch_time:0.0864\n\n2024-03-28 08:53:51,004 - vgg11_training - INFO - batch:22/160, loss:0.6352, batch_time:0.0862\n\n2024-03-28 08:53:55,119 - vgg11_training - INFO - batch:23/160, loss:0.5826, batch_time:0.0864\n\n2024-03-28 08:53:59,317 - vgg11_training - INFO - batch:24/160, loss:0.6196, batch_time:0.0865\n\n2024-03-28 08:54:03,654 - vgg11_training - INFO - batch:25/160, loss:0.5413, batch_time:0.0922\n\n2024-03-28 08:54:07,859 - vgg11_training - INFO - batch:26/160, loss:0.6301, batch_time:0.0864\n\n2024-03-28 08:54:11,949 - vgg11_training - INFO - batch:27/160, loss:0.6149, batch_time:0.0861\n\n2024-03-28 08:54:15,961 - vgg11_training - INFO - batch:28/160, loss:0.6102, batch_time:0.0860\n\n2024-03-28 08:54:20,216 - vgg11_training - INFO - batch:29/160, loss:0.5738, batch_time:0.0861\n\n2024-03-28 08:54:24,277 - vgg11_training - INFO - batch:30/160, loss:0.6104, batch_time:0.0863\n\n2024-03-28 08:54:28,156 - vgg11_training - INFO - batch:31/160, loss:0.5627, batch_time:0.0864\n\n2024-03-28 08:54:32,598 - vgg11_training - INFO - batch:32/160, loss:0.5894, batch_time:0.0862\n\n2024-03-28 08:54:36,684 - vgg11_training - INFO - batch:33/160, loss:0.6096, batch_time:0.0864\n\n2024-03-28 08:54:40,607 - vgg11_training - INFO - batch:34/160, loss:0.6331, batch_time:0.0864\n\n2024-03-28 08:54:44,551 - vgg11_training - INFO - batch:35/160, loss:0.5805, batch_time:0.0868\n\n2024-03-28 08:54:48,347 - vgg11_training - INFO - batch:36/160, loss:0.6231, batch_time:0.0921\n\n2024-03-28 08:54:52,524 - vgg11_training - INFO - batch:37/160, loss:0.6567, batch_time:0.0864\n\n2024-03-28 08:54:56,876 - vgg11_training - INFO - batch:38/160, loss:0.5812, batch_time:0.0866\n\n2024-03-28 08:55:00,629 - vgg11_training - INFO - batch:39/160, loss:0.6474, batch_time:0.0861\n\n2024-03-28 08:55:04,562 - vgg11_training - INFO - batch:40/160, loss:0.5714, batch_time:0.0864\n\n2024-03-28 08:55:08,719 - vgg11_training - INFO - batch:41/160, loss:0.5970, batch_time:0.0863\n\n2024-03-28 08:55:12,794 - vgg11_training - INFO - batch:42/160, loss:0.6050, batch_time:0.0866\n\n2024-03-28 08:55:17,132 - vgg11_training - INFO - batch:43/160, loss:0.5614, batch_time:0.0865\n\n2024-03-28 08:55:21,020 - vgg11_training - INFO - batch:44/160, loss:0.5466, batch_time:0.0866\n\n2024-03-28 08:55:24,889 - vgg11_training - INFO - batch:45/160, loss:0.5966, batch_time:0.0863\n\n2024-03-28 08:55:28,851 - vgg11_training - INFO - batch:46/160, loss:0.5569, batch_time:0.0865\n\n2024-03-28 08:55:33,089 - vgg11_training - INFO - batch:47/160, loss:0.5864, batch_time:0.0866\n\n2024-03-28 08:55:37,569 - vgg11_training - INFO - batch:48/160, loss:0.5723, batch_time:0.0924\n\n2024-03-28 08:55:41,855 - vgg11_training - INFO - batch:49/160, loss:0.5902, batch_time:0.0869\n\n2024-03-28 08:55:46,079 - vgg11_training - INFO - batch:50/160, loss:0.5514, batch_time:0.0863\n\n2024-03-28 08:55:50,164 - vgg11_training - INFO - batch:51/160, loss:0.5204, batch_time:0.0862\n\n2024-03-28 08:55:54,061 - vgg11_training - INFO - batch:52/160, loss:0.6415, batch_time:0.0865\n\n2024-03-28 08:55:57,939 - vgg11_training - INFO - batch:53/160, loss:0.5119, batch_time:0.0863\n\n2024-03-28 08:56:01,933 - vgg11_training - INFO - batch:54/160, loss:0.5614, batch_time:0.0866\n\n2024-03-28 08:56:05,909 - vgg11_training - INFO - batch:55/160, loss:0.5723, batch_time:0.0869\n\n2024-03-28 08:56:09,758 - vgg11_training - INFO - batch:56/160, loss:0.4712, batch_time:0.0865\n\n2024-03-28 08:56:13,880 - vgg11_training - INFO - batch:57/160, loss:0.5000, batch_time:0.0864\n\n2024-03-28 08:56:17,850 - vgg11_training - INFO - batch:58/160, loss:0.5054, batch_time:0.0862\n\n2024-03-28 08:56:21,879 - vgg11_training - INFO - batch:59/160, loss:0.5443, batch_time:0.0926\n\n2024-03-28 08:56:25,729 - vgg11_training - INFO - batch:60/160, loss:0.4762, batch_time:0.0868\n\n2024-03-28 08:56:29,746 - vgg11_training - INFO - batch:61/160, loss:0.5375, batch_time:0.0864\n\n2024-03-28 08:56:34,043 - vgg11_training - INFO - batch:62/160, loss:0.5558, batch_time:0.0862\n\n2024-03-28 08:56:38,105 - vgg11_training - INFO - batch:63/160, loss:0.5655, batch_time:0.0862\n\n2024-03-28 08:56:42,212 - vgg11_training - INFO - batch:64/160, loss:0.5183, batch_time:0.0860\n\n2024-03-28 08:56:46,004 - vgg11_training - INFO - batch:65/160, loss:0.5730, batch_time:0.0863\n\n2024-03-28 08:56:50,173 - vgg11_training - INFO - batch:66/160, loss:0.4513, batch_time:0.0864\n\n2024-03-28 08:56:54,555 - vgg11_training - INFO - batch:67/160, loss:0.4531, batch_time:0.0863\n\n2024-03-28 08:56:58,342 - vgg11_training - INFO - batch:68/160, loss:0.5073, batch_time:0.0863\n\n2024-03-28 08:57:02,616 - vgg11_training - INFO - batch:69/160, loss:0.4389, batch_time:0.0861\n\n2024-03-28 08:57:06,668 - vgg11_training - INFO - batch:70/160, loss:0.4601, batch_time:0.0923\n\n2024-03-28 08:57:10,451 - vgg11_training - INFO - batch:71/160, loss:0.4601, batch_time:0.0870\n\n2024-03-28 08:57:14,362 - vgg11_training - INFO - batch:72/160, loss:0.4125, batch_time:0.0866\n\n2024-03-28 08:57:18,660 - vgg11_training - INFO - batch:73/160, loss:0.4363, batch_time:0.0864\n\n2024-03-28 08:57:22,874 - vgg11_training - INFO - batch:74/160, loss:0.3722, batch_time:0.0864\n\n2024-03-28 08:57:26,928 - vgg11_training - INFO - batch:75/160, loss:0.4903, batch_time:0.0862\n\n2024-03-28 08:57:31,710 - vgg11_training - INFO - batch:76/160, loss:0.3889, batch_time:0.0861\n\n2024-03-28 08:57:35,412 - vgg11_training - INFO - batch:77/160, loss:0.4080, batch_time:0.0862\n\n2024-03-28 08:57:39,399 - vgg11_training - INFO - batch:78/160, loss:0.4012, batch_time:0.0880\n\n2024-03-28 08:57:43,492 - vgg11_training - INFO - batch:79/160, loss:0.4181, batch_time:0.0868\n\n2024-03-28 08:57:47,165 - vgg11_training - INFO - batch:80/160, loss:0.4152, batch_time:0.0870\n\n2024-03-28 08:57:51,320 - vgg11_training - INFO - batch:81/160, loss:0.3131, batch_time:0.0859\n\n2024-03-28 08:57:55,713 - vgg11_training - INFO - batch:82/160, loss:0.3790, batch_time:0.0921\n\n2024-03-28 08:57:59,433 - vgg11_training - INFO - batch:83/160, loss:0.3153, batch_time:0.0862\n\n2024-03-28 08:58:03,532 - vgg11_training - INFO - batch:84/160, loss:0.4276, batch_time:0.0863\n\n2024-03-28 08:58:07,919 - vgg11_training - INFO - batch:85/160, loss:0.3180, batch_time:0.0866\n\n2024-03-28 08:58:11,983 - vgg11_training - INFO - batch:86/160, loss:0.4256, batch_time:0.0862\n\n2024-03-28 08:58:15,960 - vgg11_training - INFO - batch:87/160, loss:0.2949, batch_time:0.0866\n\n2024-03-28 08:58:19,837 - vgg11_training - INFO - batch:88/160, loss:0.3398, batch_time:0.0863\n\n2024-03-28 08:58:24,161 - vgg11_training - INFO - batch:89/160, loss:0.2742, batch_time:0.0860\n\n2024-03-28 08:58:28,160 - vgg11_training - INFO - batch:90/160, loss:0.2627, batch_time:0.0862\n\n2024-03-28 08:58:32,138 - vgg11_training - INFO - batch:91/160, loss:0.2593, batch_time:0.0861\n\n2024-03-28 08:58:36,476 - vgg11_training - INFO - batch:92/160, loss:0.2971, batch_time:0.0866\n\n2024-03-28 08:58:40,556 - vgg11_training - INFO - batch:93/160, loss:0.2250, batch_time:0.0920\n\n2024-03-28 08:58:44,585 - vgg11_training - INFO - batch:94/160, loss:0.2475, batch_time:0.0862\n\n2024-03-28 08:58:48,811 - vgg11_training - INFO - batch:95/160, loss:0.2509, batch_time:0.0867\n\n2024-03-28 08:58:53,545 - vgg11_training - INFO - batch:96/160, loss:0.2037, batch_time:0.0864\n\n2024-03-28 08:58:57,714 - vgg11_training - INFO - batch:97/160, loss:0.2375, batch_time:0.0873\n\n2024-03-28 08:59:01,772 - vgg11_training - INFO - batch:98/160, loss:0.1794, batch_time:0.0862\n\n2024-03-28 08:59:05,899 - vgg11_training - INFO - batch:99/160, loss:0.2027, batch_time:0.0865\n\n2024-03-28 08:59:10,116 - vgg11_training - INFO - batch:100/160, loss:0.2552, batch_time:0.0864\n\n2024-03-28 08:59:14,426 - vgg11_training - INFO - batch:101/160, loss:0.3142, batch_time:0.0859\n\n2024-03-28 08:59:18,780 - vgg11_training - INFO - batch:102/160, loss:0.2452, batch_time:0.0863\n\n2024-03-28 08:59:23,021 - vgg11_training - INFO - batch:103/160, loss:0.2804, batch_time:0.0874\n\n2024-03-28 08:59:27,336 - vgg11_training - INFO - batch:104/160, loss:0.1616, batch_time:0.0925\n\n2024-03-28 08:59:31,708 - vgg11_training - INFO - batch:105/160, loss:0.2100, batch_time:0.0863\n\n2024-03-28 08:59:35,893 - vgg11_training - INFO - batch:106/160, loss:0.1579, batch_time:0.0866\n\n2024-03-28 08:59:40,003 - vgg11_training - INFO - batch:107/160, loss:0.3449, batch_time:0.0868\n\n2024-03-28 08:59:44,220 - vgg11_training - INFO - batch:108/160, loss:0.2744, batch_time:0.0861\n\n2024-03-28 08:59:48,211 - vgg11_training - INFO - batch:109/160, loss:0.2012, batch_time:0.0862\n\n2024-03-28 08:59:52,330 - vgg11_training - INFO - batch:110/160, loss:0.2441, batch_time:0.0866\n\n2024-03-28 08:59:56,682 - vgg11_training - INFO - batch:111/160, loss:0.1603, batch_time:0.0862\n\n2024-03-28 09:00:01,006 - vgg11_training - INFO - batch:112/160, loss:0.2065, batch_time:0.0859\n\n2024-03-28 09:00:05,417 - vgg11_training - INFO - batch:113/160, loss:0.1770, batch_time:0.0867\n\n2024-03-28 09:00:09,901 - vgg11_training - INFO - batch:114/160, loss:0.1721, batch_time:0.0869\n\n2024-03-28 09:00:14,086 - vgg11_training - INFO - batch:115/160, loss:0.1616, batch_time:0.0923\n\n2024-03-28 09:00:18,094 - vgg11_training - INFO - batch:116/160, loss:0.2534, batch_time:0.0866\n\n2024-03-28 09:00:22,037 - vgg11_training - INFO - batch:117/160, loss:0.2633, batch_time:0.0863\n\n2024-03-28 09:00:26,432 - vgg11_training - INFO - batch:118/160, loss:0.1023, batch_time:0.0871\n\n2024-03-28 09:00:30,623 - vgg11_training - INFO - batch:119/160, loss:0.1451, batch_time:0.0863\n\n2024-03-28 09:00:34,346 - vgg11_training - INFO - batch:120/160, loss:0.1911, batch_time:0.0862\n\n2024-03-28 09:00:38,248 - vgg11_training - INFO - batch:121/160, loss:0.2022, batch_time:0.0862\n\n2024-03-28 09:00:42,241 - vgg11_training - INFO - batch:122/160, loss:0.1965, batch_time:0.0862\n\n2024-03-28 09:00:46,106 - vgg11_training - INFO - batch:123/160, loss:0.2843, batch_time:0.0863\n\n2024-03-28 09:00:49,763 - vgg11_training - INFO - batch:124/160, loss:0.2019, batch_time:0.0862\n\n2024-03-28 09:00:54,055 - vgg11_training - INFO - batch:125/160, loss:0.1614, batch_time:0.0866\n\n2024-03-28 09:00:57,983 - vgg11_training - INFO - batch:126/160, loss:0.2190, batch_time:0.0865\n\n2024-03-28 09:01:02,073 - vgg11_training - INFO - batch:127/160, loss:0.1468, batch_time:0.0921\n\n2024-03-28 09:01:05,829 - vgg11_training - INFO - batch:128/160, loss:0.1615, batch_time:0.0866\n\n2024-03-28 09:01:09,779 - vgg11_training - INFO - batch:129/160, loss:0.2089, batch_time:0.0865\n\n2024-03-28 09:01:14,105 - vgg11_training - INFO - batch:130/160, loss:0.1583, batch_time:0.0866\n\n2024-03-28 09:01:18,844 - vgg11_training - INFO - batch:131/160, loss:0.1058, batch_time:0.0864\n\n2024-03-28 09:01:23,106 - vgg11_training - INFO - batch:132/160, loss:0.1302, batch_time:0.0866\n\n2024-03-28 09:01:27,290 - vgg11_training - INFO - batch:133/160, loss:0.1282, batch_time:0.0866\n\n2024-03-28 09:01:31,689 - vgg11_training - INFO - batch:134/160, loss:0.1309, batch_time:0.0861\n\n2024-03-28 09:01:35,810 - vgg11_training - INFO - batch:135/160, loss:0.0827, batch_time:0.0864\n\n2024-03-28 09:01:39,808 - vgg11_training - INFO - batch:136/160, loss:0.1062, batch_time:0.0864\n\n2024-03-28 09:01:44,015 - vgg11_training - INFO - batch:137/160, loss:0.1508, batch_time:0.0868\n\n2024-03-28 09:01:48,011 - vgg11_training - INFO - batch:138/160, loss:0.2436, batch_time:0.0921\n\n2024-03-28 09:01:52,123 - vgg11_training - INFO - batch:139/160, loss:0.1981, batch_time:0.0864\n\n2024-03-28 09:01:56,596 - vgg11_training - INFO - batch:140/160, loss:0.2352, batch_time:0.0863\n\n2024-03-28 09:02:00,627 - vgg11_training - INFO - batch:141/160, loss:0.2219, batch_time:0.0861\n\n2024-03-28 09:02:04,667 - vgg11_training - INFO - batch:142/160, loss:0.1948, batch_time:0.0860\n\n2024-03-28 09:02:08,728 - vgg11_training - INFO - batch:143/160, loss:0.0466, batch_time:0.0861\n\n2024-03-28 09:02:12,960 - vgg11_training - INFO - batch:144/160, loss:0.1714, batch_time:0.0864\n\n2024-03-28 09:02:17,082 - vgg11_training - INFO - batch:145/160, loss:0.1721, batch_time:0.0863\n\n2024-03-28 09:02:21,512 - vgg11_training - INFO - batch:146/160, loss:0.1463, batch_time:0.0862\n\n2024-03-28 09:02:25,479 - vgg11_training - INFO - batch:147/160, loss:0.1420, batch_time:0.0862\n\n2024-03-28 09:02:29,587 - vgg11_training - INFO - batch:148/160, loss:0.1073, batch_time:0.0867\n\n2024-03-28 09:02:33,765 - vgg11_training - INFO - batch:149/160, loss:0.0984, batch_time:0.0923\n\n2024-03-28 09:02:37,973 - vgg11_training - INFO - batch:150/160, loss:0.2910, batch_time:0.0862\n\n2024-03-28 09:02:42,108 - vgg11_training - INFO - batch:151/160, loss:0.1333, batch_time:0.0864\n\n2024-03-28 09:02:46,023 - vgg11_training - INFO - batch:152/160, loss:0.1590, batch_time:0.0861\n\n2024-03-28 09:02:49,968 - vgg11_training - INFO - batch:153/160, loss:0.1773, batch_time:0.0863\n\n2024-03-28 09:02:53,937 - vgg11_training - INFO - batch:154/160, loss:0.0966, batch_time:0.0865\n\n2024-03-28 09:02:58,270 - vgg11_training - INFO - batch:155/160, loss:0.2572, batch_time:0.0862\n\n2024-03-28 09:03:02,613 - vgg11_training - INFO - batch:156/160, loss:0.2042, batch_time:0.0870\n\n2024-03-28 09:03:06,586 - vgg11_training - INFO - batch:157/160, loss:0.1764, batch_time:0.0863\n\n2024-03-28 09:03:10,518 - vgg11_training - INFO - batch:158/160, loss:0.1011, batch_time:0.0865\n\n2024-03-28 09:03:14,248 - vgg11_training - INFO - batch:159/160, loss:0.1234, batch_time:0.0862\n\n2024-03-28 09:03:17,635 - vgg11_training - INFO - batch:160/160, loss:0.1146, batch_time:0.0890\n\n2024-03-28 09:03:17,636 - vgg11_training - INFO - Epoch 1/20, total_loss: 62.4307\n\n2024-03-28 09:03:18,582 - vgg11_training - INFO - Test_epoch:1/20\n\n2024-03-28 09:05:56,078 - vgg11_training - INFO - Test Accuracy: 0.9624, Test Precision: 0.9605, Test Recall: 0.9611, Test F1: 0.9608 Test time: 157.49417853355408\n\n2024-03-28 09:05:56,079 - vgg11_training - INFO - epoch:1/20, total_time:0:13:35\n\n2024-03-28 09:05:56,081 - vgg11_training - INFO - Train_epoch:2/20\n\n2024-03-28 09:05:59,462 - vgg11_training - INFO - batch:1/160, loss:0.1535, batch_time:0.0870\n\n2024-03-28 09:06:02,857 - vgg11_training - INFO - batch:2/160, loss:0.0625, batch_time:0.0864\n\n2024-03-28 09:06:06,289 - vgg11_training - INFO - batch:3/160, loss:0.1509, batch_time:0.0864\n\n2024-03-28 09:06:09,641 - vgg11_training - INFO - batch:4/160, loss:0.1490, batch_time:0.0863\n\n2024-03-28 09:06:13,216 - vgg11_training - INFO - batch:5/160, loss:0.1530, batch_time:0.0863\n\n2024-03-28 09:06:16,747 - vgg11_training - INFO - batch:6/160, loss:0.1258, batch_time:0.0862\n\n2024-03-28 09:06:20,232 - vgg11_training - INFO - batch:7/160, loss:0.0774, batch_time:0.0859\n\n2024-03-28 09:06:24,125 - vgg11_training - INFO - batch:8/160, loss:0.1151, batch_time:0.0921\n\n2024-03-28 09:06:27,645 - vgg11_training - INFO - batch:9/160, loss:0.0851, batch_time:0.0863\n\n2024-03-28 09:06:31,167 - vgg11_training - INFO - batch:10/160, loss:0.0635, batch_time:0.0861\n\n2024-03-28 09:06:34,669 - vgg11_training - INFO - batch:11/160, loss:0.0795, batch_time:0.0864\n\n2024-03-28 09:06:38,065 - vgg11_training - INFO - batch:12/160, loss:0.0742, batch_time:0.0861\n\n2024-03-28 09:06:41,458 - vgg11_training - INFO - batch:13/160, loss:0.0797, batch_time:0.0867\n\n2024-03-28 09:06:45,319 - vgg11_training - INFO - batch:14/160, loss:0.1209, batch_time:0.0863\n\n2024-03-28 09:06:48,795 - vgg11_training - INFO - batch:15/160, loss:0.0707, batch_time:0.0864\n\n2024-03-28 09:06:52,403 - vgg11_training - INFO - batch:16/160, loss:0.0962, batch_time:0.0867\n\n2024-03-28 09:06:56,085 - vgg11_training - INFO - batch:17/160, loss:0.1418, batch_time:0.0864\n\n2024-03-28 09:06:59,673 - vgg11_training - INFO - batch:18/160, loss:0.1034, batch_time:0.0860\n\n2024-03-28 09:07:03,145 - vgg11_training - INFO - batch:19/160, loss:0.1528, batch_time:0.0861\n\n2024-03-28 09:07:06,543 - vgg11_training - INFO - batch:20/160, loss:0.1527, batch_time:0.0862\n\n2024-03-28 09:07:10,002 - vgg11_training - INFO - batch:21/160, loss:0.1040, batch_time:0.0921\n\n2024-03-28 09:07:13,520 - vgg11_training - INFO - batch:22/160, loss:0.0961, batch_time:0.0866\n\n2024-03-28 09:07:16,882 - vgg11_training - INFO - batch:23/160, loss:0.1637, batch_time:0.0861\n\n2024-03-28 09:07:20,257 - vgg11_training - INFO - batch:24/160, loss:0.0888, batch_time:0.0866\n\n2024-03-28 09:07:23,861 - vgg11_training - INFO - batch:25/160, loss:0.0443, batch_time:0.0861\n\n2024-03-28 09:07:27,484 - vgg11_training - INFO - batch:26/160, loss:0.0386, batch_time:0.0864\n\n2024-03-28 09:07:31,099 - vgg11_training - INFO - batch:27/160, loss:0.1486, batch_time:0.0863\n\n2024-03-28 09:07:34,476 - vgg11_training - INFO - batch:28/160, loss:0.1715, batch_time:0.0867\n\n2024-03-28 09:07:38,048 - vgg11_training - INFO - batch:29/160, loss:0.1031, batch_time:0.0863\n\n2024-03-28 09:07:41,765 - vgg11_training - INFO - batch:30/160, loss:0.1410, batch_time:0.0866\n\n2024-03-28 09:07:45,530 - vgg11_training - INFO - batch:31/160, loss:0.1176, batch_time:0.0863\n\n2024-03-28 09:07:49,142 - vgg11_training - INFO - batch:32/160, loss:0.0985, batch_time:0.0865\n\n2024-03-28 09:07:52,664 - vgg11_training - INFO - batch:33/160, loss:0.0881, batch_time:0.0873\n\n2024-03-28 09:07:56,235 - vgg11_training - INFO - batch:34/160, loss:0.1464, batch_time:0.0925\n\n2024-03-28 09:07:59,917 - vgg11_training - INFO - batch:35/160, loss:0.2007, batch_time:0.0868\n\n2024-03-28 09:08:03,144 - vgg11_training - INFO - batch:36/160, loss:0.0954, batch_time:0.0862\n\n2024-03-28 09:08:06,538 - vgg11_training - INFO - batch:37/160, loss:0.0962, batch_time:0.0866\n\n2024-03-28 09:08:10,361 - vgg11_training - INFO - batch:38/160, loss:0.0714, batch_time:0.0866\n\n2024-03-28 09:08:13,998 - vgg11_training - INFO - batch:39/160, loss:0.0609, batch_time:0.0864\n\n2024-03-28 09:08:17,378 - vgg11_training - INFO - batch:40/160, loss:0.0647, batch_time:0.0861\n\n2024-03-28 09:08:20,878 - vgg11_training - INFO - batch:41/160, loss:0.1614, batch_time:0.0865\n\n2024-03-28 09:08:24,316 - vgg11_training - INFO - batch:42/160, loss:0.0738, batch_time:0.0865\n\n2024-03-28 09:08:27,951 - vgg11_training - INFO - batch:43/160, loss:0.1943, batch_time:0.0862\n\n2024-03-28 09:08:31,315 - vgg11_training - INFO - batch:44/160, loss:0.0556, batch_time:0.0860\n\n2024-03-28 09:08:34,955 - vgg11_training - INFO - batch:45/160, loss:0.1448, batch_time:0.0863\n\n2024-03-28 09:08:38,431 - vgg11_training - INFO - batch:46/160, loss:0.1089, batch_time:0.0862\n\n2024-03-28 09:08:42,049 - vgg11_training - INFO - batch:47/160, loss:0.0402, batch_time:0.0938\n\n2024-03-28 09:08:45,956 - vgg11_training - INFO - batch:48/160, loss:0.1168, batch_time:0.0864\n\n2024-03-28 09:08:49,747 - vgg11_training - INFO - batch:49/160, loss:0.0931, batch_time:0.0862\n\n2024-03-28 09:08:53,266 - vgg11_training - INFO - batch:50/160, loss:0.0361, batch_time:0.0861\n\n2024-03-28 09:08:57,109 - vgg11_training - INFO - batch:51/160, loss:0.1524, batch_time:0.0862\n\n2024-03-28 09:09:00,627 - vgg11_training - INFO - batch:52/160, loss:0.0743, batch_time:0.0861\n\n2024-03-28 09:09:04,172 - vgg11_training - INFO - batch:53/160, loss:0.0537, batch_time:0.0864\n\n2024-03-28 09:09:07,538 - vgg11_training - INFO - batch:54/160, loss:0.0605, batch_time:0.0867\n\n2024-03-28 09:09:11,124 - vgg11_training - INFO - batch:55/160, loss:0.0685, batch_time:0.0866\n\n2024-03-28 09:09:14,502 - vgg11_training - INFO - batch:56/160, loss:0.0748, batch_time:0.0864\n\n2024-03-28 09:09:17,961 - vgg11_training - INFO - batch:57/160, loss:0.1152, batch_time:0.0902\n\n2024-03-28 09:09:21,796 - vgg11_training - INFO - batch:58/160, loss:0.0474, batch_time:0.0867\n\n2024-03-28 09:09:25,517 - vgg11_training - INFO - batch:59/160, loss:0.1022, batch_time:0.0867\n\n2024-03-28 09:09:28,942 - vgg11_training - INFO - batch:60/160, loss:0.0737, batch_time:0.0927\n\n2024-03-28 09:09:32,364 - vgg11_training - INFO - batch:61/160, loss:0.1334, batch_time:0.0866\n\n2024-03-28 09:09:36,018 - vgg11_training - INFO - batch:62/160, loss:0.0263, batch_time:0.0865\n\n2024-03-28 09:09:39,660 - vgg11_training - INFO - batch:63/160, loss:0.0433, batch_time:0.0865\n\n2024-03-28 09:09:43,251 - vgg11_training - INFO - batch:64/160, loss:0.2029, batch_time:0.0867\n\n2024-03-28 09:09:46,840 - vgg11_training - INFO - batch:65/160, loss:0.0753, batch_time:0.0866\n\n2024-03-28 09:09:50,340 - vgg11_training - INFO - batch:66/160, loss:0.0777, batch_time:0.0865\n\n2024-03-28 09:09:53,871 - vgg11_training - INFO - batch:67/160, loss:0.1633, batch_time:0.0867\n\n2024-03-28 09:09:57,307 - vgg11_training - INFO - batch:68/160, loss:0.0646, batch_time:0.0860\n\n2024-03-28 09:10:00,758 - vgg11_training - INFO - batch:69/160, loss:0.0584, batch_time:0.0864\n\n2024-03-28 09:10:04,146 - vgg11_training - INFO - batch:70/160, loss:0.1252, batch_time:0.0862\n\n2024-03-28 09:10:07,945 - vgg11_training - INFO - batch:71/160, loss:0.0703, batch_time:0.0869\n\n2024-03-28 09:10:11,371 - vgg11_training - INFO - batch:72/160, loss:0.0607, batch_time:0.0863\n\n2024-03-28 09:10:14,823 - vgg11_training - INFO - batch:73/160, loss:0.1068, batch_time:0.0923\n\n2024-03-28 09:10:18,779 - vgg11_training - INFO - batch:74/160, loss:0.1203, batch_time:0.0863\n\n2024-03-28 09:10:22,756 - vgg11_training - INFO - batch:75/160, loss:0.0840, batch_time:0.0864\n\n2024-03-28 09:10:26,492 - vgg11_training - INFO - batch:76/160, loss:0.1296, batch_time:0.0864\n\n2024-03-28 09:10:29,893 - vgg11_training - INFO - batch:77/160, loss:0.0952, batch_time:0.0860\n\n2024-03-28 09:10:33,317 - vgg11_training - INFO - batch:78/160, loss:0.1323, batch_time:0.0864\n\n2024-03-28 09:10:36,741 - vgg11_training - INFO - batch:79/160, loss:0.1165, batch_time:0.0862\n\n2024-03-28 09:10:40,446 - vgg11_training - INFO - batch:80/160, loss:0.0663, batch_time:0.0864\n\n2024-03-28 09:10:43,734 - vgg11_training - INFO - batch:81/160, loss:0.0415, batch_time:0.0862\n\n2024-03-28 09:10:47,267 - vgg11_training - INFO - batch:82/160, loss:0.0457, batch_time:0.0865\n\n2024-03-28 09:10:51,086 - vgg11_training - INFO - batch:83/160, loss:0.0711, batch_time:0.0861\n\n2024-03-28 09:10:54,723 - vgg11_training - INFO - batch:84/160, loss:0.0925, batch_time:0.0862\n\n2024-03-28 09:10:58,324 - vgg11_training - INFO - batch:85/160, loss:0.0636, batch_time:0.0863\n\n2024-03-28 09:11:01,856 - vgg11_training - INFO - batch:86/160, loss:0.0439, batch_time:0.0918\n\n2024-03-28 09:11:05,568 - vgg11_training - INFO - batch:87/160, loss:0.0282, batch_time:0.0865\n\n2024-03-28 09:11:09,551 - vgg11_training - INFO - batch:88/160, loss:0.0442, batch_time:0.0861\n\n2024-03-28 09:11:12,943 - vgg11_training - INFO - batch:89/160, loss:0.0455, batch_time:0.0863\n\n2024-03-28 09:11:16,386 - vgg11_training - INFO - batch:90/160, loss:0.0179, batch_time:0.0868\n\n2024-03-28 09:11:20,064 - vgg11_training - INFO - batch:91/160, loss:0.0816, batch_time:0.0866\n\n2024-03-28 09:11:23,760 - vgg11_training - INFO - batch:92/160, loss:0.0344, batch_time:0.0869\n\n2024-03-28 09:11:27,355 - vgg11_training - INFO - batch:93/160, loss:0.1162, batch_time:0.0868\n\n2024-03-28 09:11:31,072 - vgg11_training - INFO - batch:94/160, loss:0.0337, batch_time:0.0862\n\n2024-03-28 09:11:34,476 - vgg11_training - INFO - batch:95/160, loss:0.0172, batch_time:0.0866\n\n2024-03-28 09:11:38,305 - vgg11_training - INFO - batch:96/160, loss:0.0118, batch_time:0.0863\n\n2024-03-28 09:11:41,866 - vgg11_training - INFO - batch:97/160, loss:0.1271, batch_time:0.0865\n\n2024-03-28 09:11:45,313 - vgg11_training - INFO - batch:98/160, loss:0.1715, batch_time:0.0869\n\n2024-03-28 09:11:49,018 - vgg11_training - INFO - batch:99/160, loss:0.0275, batch_time:0.0923\n\n2024-03-28 09:11:52,698 - vgg11_training - INFO - batch:100/160, loss:0.0744, batch_time:0.0860\n\n2024-03-28 09:11:56,499 - vgg11_training - INFO - batch:101/160, loss:0.0275, batch_time:0.0862\n\n2024-03-28 09:12:00,160 - vgg11_training - INFO - batch:102/160, loss:0.0461, batch_time:0.0865\n\n2024-03-28 09:12:03,790 - vgg11_training - INFO - batch:103/160, loss:0.0445, batch_time:0.0865\n\n2024-03-28 09:12:07,430 - vgg11_training - INFO - batch:104/160, loss:0.0542, batch_time:0.0868\n\n2024-03-28 09:12:10,839 - vgg11_training - INFO - batch:105/160, loss:0.0281, batch_time:0.0864\n\n2024-03-28 09:12:14,515 - vgg11_training - INFO - batch:106/160, loss:0.0734, batch_time:0.0867\n\n2024-03-28 09:12:18,085 - vgg11_training - INFO - batch:107/160, loss:0.0353, batch_time:0.0863\n\n2024-03-28 09:12:21,861 - vgg11_training - INFO - batch:108/160, loss:0.0687, batch_time:0.0862\n\n2024-03-28 09:12:25,327 - vgg11_training - INFO - batch:109/160, loss:0.2893, batch_time:0.0864\n\n2024-03-28 09:12:29,104 - vgg11_training - INFO - batch:110/160, loss:0.0910, batch_time:0.0883\n\n2024-03-28 09:12:33,109 - vgg11_training - INFO - batch:111/160, loss:0.0237, batch_time:0.0865\n\n2024-03-28 09:12:36,519 - vgg11_training - INFO - batch:112/160, loss:0.0355, batch_time:0.0925\n\n2024-03-28 09:12:40,019 - vgg11_training - INFO - batch:113/160, loss:0.0618, batch_time:0.0864\n\n2024-03-28 09:12:43,555 - vgg11_training - INFO - batch:114/160, loss:0.0618, batch_time:0.0864\n\n2024-03-28 09:12:47,271 - vgg11_training - INFO - batch:115/160, loss:0.0381, batch_time:0.0871\n\n2024-03-28 09:12:50,876 - vgg11_training - INFO - batch:116/160, loss:0.0304, batch_time:0.0867\n\n2024-03-28 09:12:54,270 - vgg11_training - INFO - batch:117/160, loss:0.0314, batch_time:0.0865\n\n2024-03-28 09:12:57,760 - vgg11_training - INFO - batch:118/160, loss:0.0434, batch_time:0.0861\n\n2024-03-28 09:13:01,601 - vgg11_training - INFO - batch:119/160, loss:0.0335, batch_time:0.0867\n\n2024-03-28 09:13:05,217 - vgg11_training - INFO - batch:120/160, loss:0.1594, batch_time:0.0868\n\n2024-03-28 09:13:08,877 - vgg11_training - INFO - batch:121/160, loss:0.0323, batch_time:0.0865\n\n2024-03-28 09:13:12,466 - vgg11_training - INFO - batch:122/160, loss:0.0266, batch_time:0.0862\n\n2024-03-28 09:13:16,497 - vgg11_training - INFO - batch:123/160, loss:0.0364, batch_time:0.0867\n\n2024-03-28 09:13:20,310 - vgg11_training - INFO - batch:124/160, loss:0.1333, batch_time:0.0862\n\n2024-03-28 09:13:23,935 - vgg11_training - INFO - batch:125/160, loss:0.1314, batch_time:0.0925\n\n2024-03-28 09:13:27,741 - vgg11_training - INFO - batch:126/160, loss:0.1464, batch_time:0.0867\n\n2024-03-28 09:13:31,563 - vgg11_training - INFO - batch:127/160, loss:0.0867, batch_time:0.0863\n\n2024-03-28 09:13:35,060 - vgg11_training - INFO - batch:128/160, loss:0.0448, batch_time:0.0867\n\n2024-03-28 09:13:38,680 - vgg11_training - INFO - batch:129/160, loss:0.1625, batch_time:0.0868\n\n2024-03-28 09:13:42,192 - vgg11_training - INFO - batch:130/160, loss:0.0317, batch_time:0.0862\n\n2024-03-28 09:13:45,782 - vgg11_training - INFO - batch:131/160, loss:0.0330, batch_time:0.0865\n\n2024-03-28 09:13:49,045 - vgg11_training - INFO - batch:132/160, loss:0.0233, batch_time:0.0865\n\n2024-03-28 09:13:52,638 - vgg11_training - INFO - batch:133/160, loss:0.1766, batch_time:0.0864\n\n2024-03-28 09:13:56,173 - vgg11_training - INFO - batch:134/160, loss:0.1059, batch_time:0.0861\n\n2024-03-28 09:14:00,001 - vgg11_training - INFO - batch:135/160, loss:0.0269, batch_time:0.0874\n\n2024-03-28 09:14:03,681 - vgg11_training - INFO - batch:136/160, loss:0.0872, batch_time:0.0861\n\n2024-03-28 09:14:07,310 - vgg11_training - INFO - batch:137/160, loss:0.1153, batch_time:0.0862\n\n2024-03-28 09:14:10,762 - vgg11_training - INFO - batch:138/160, loss:0.0828, batch_time:0.0923\n\n2024-03-28 09:14:14,359 - vgg11_training - INFO - batch:139/160, loss:0.0183, batch_time:0.0867\n\n2024-03-28 09:14:18,089 - vgg11_training - INFO - batch:140/160, loss:0.0347, batch_time:0.0866\n\n2024-03-28 09:14:21,712 - vgg11_training - INFO - batch:141/160, loss:0.0750, batch_time:0.0866\n\n2024-03-28 09:14:25,295 - vgg11_training - INFO - batch:142/160, loss:0.1751, batch_time:0.0866\n\n2024-03-28 09:14:29,029 - vgg11_training - INFO - batch:143/160, loss:0.0771, batch_time:0.0865\n\n2024-03-28 09:14:32,693 - vgg11_training - INFO - batch:144/160, loss:0.0308, batch_time:0.0861\n\n2024-03-28 09:14:36,367 - vgg11_training - INFO - batch:145/160, loss:0.0449, batch_time:0.0861\n\n2024-03-28 09:14:40,005 - vgg11_training - INFO - batch:146/160, loss:0.0631, batch_time:0.0864\n\n2024-03-28 09:14:44,069 - vgg11_training - INFO - batch:147/160, loss:0.0181, batch_time:0.0867\n\n2024-03-28 09:14:47,750 - vgg11_training - INFO - batch:148/160, loss:0.0554, batch_time:0.0861\n\n2024-03-28 09:14:51,371 - vgg11_training - INFO - batch:149/160, loss:0.1069, batch_time:0.0866\n\n2024-03-28 09:14:54,806 - vgg11_training - INFO - batch:150/160, loss:0.1189, batch_time:0.0862\n\n2024-03-28 09:14:58,344 - vgg11_training - INFO - batch:151/160, loss:0.0233, batch_time:0.0922\n\n2024-03-28 09:15:01,652 - vgg11_training - INFO - batch:152/160, loss:0.1376, batch_time:0.0862\n\n2024-03-28 09:15:05,455 - vgg11_training - INFO - batch:153/160, loss:0.2049, batch_time:0.0869\n\n2024-03-28 09:15:09,014 - vgg11_training - INFO - batch:154/160, loss:0.0224, batch_time:0.0864\n\n2024-03-28 09:15:12,606 - vgg11_training - INFO - batch:155/160, loss:0.1341, batch_time:0.0859\n\n2024-03-28 09:15:16,389 - vgg11_training - INFO - batch:156/160, loss:0.0998, batch_time:0.0864\n\n2024-03-28 09:15:19,828 - vgg11_training - INFO - batch:157/160, loss:0.0392, batch_time:0.0862\n\n2024-03-28 09:15:23,232 - vgg11_training - INFO - batch:158/160, loss:0.0607, batch_time:0.0863\n\n2024-03-28 09:15:26,950 - vgg11_training - INFO - batch:159/160, loss:0.0249, batch_time:0.0862\n\n2024-03-28 09:15:29,969 - vgg11_training - INFO - batch:160/160, loss:0.0298, batch_time:0.0777\n\n2024-03-28 09:15:29,970 - vgg11_training - INFO - Epoch 2/20, total_loss: 13.6924\n\n2024-03-28 09:15:30,920 - vgg11_training - INFO - Test_epoch:2/20\n\n2024-03-28 09:17:46,583 - vgg11_training - INFO - Test Accuracy: 0.9796, Test Precision: 0.9788, Test Recall: 0.9788, Test F1: 0.9788 Test time: 135.66198420524597\n\n2024-03-28 09:17:46,584 - vgg11_training - INFO - epoch:2/20, total_time:0:11:50\n\n2024-03-28 09:17:46,587 - vgg11_training - INFO - Train_epoch:3/20\n\n2024-03-28 09:17:50,216 - vgg11_training - INFO - batch:1/160, loss:0.0542, batch_time:0.0872\n\n2024-03-28 09:17:53,671 - vgg11_training - INFO - batch:2/160, loss:0.0142, batch_time:0.0860\n\n2024-03-28 09:17:57,286 - vgg11_training - INFO - batch:3/160, loss:0.0643, batch_time:0.0866\n\n2024-03-28 09:18:00,766 - vgg11_training - INFO - batch:4/160, loss:0.0721, batch_time:0.0865\n\n2024-03-28 09:18:04,405 - vgg11_training - INFO - batch:5/160, loss:0.0385, batch_time:0.0923\n\n2024-03-28 09:18:07,854 - vgg11_training - INFO - batch:6/160, loss:0.0218, batch_time:0.0862\n\n2024-03-28 09:18:11,467 - vgg11_training - INFO - batch:7/160, loss:0.0623, batch_time:0.0865\n\n2024-03-28 09:18:15,326 - vgg11_training - INFO - batch:8/160, loss:0.0168, batch_time:0.0865\n\n2024-03-28 09:18:18,690 - vgg11_training - INFO - batch:9/160, loss:0.1052, batch_time:0.0863\n\n2024-03-28 09:18:22,255 - vgg11_training - INFO - batch:10/160, loss:0.0328, batch_time:0.0864\n\n2024-03-28 09:18:25,744 - vgg11_training - INFO - batch:11/160, loss:0.0910, batch_time:0.0875\n\n2024-03-28 09:18:29,372 - vgg11_training - INFO - batch:12/160, loss:0.0242, batch_time:0.0860\n\n2024-03-28 09:18:32,576 - vgg11_training - INFO - batch:13/160, loss:0.0197, batch_time:0.0862\n\n2024-03-28 09:18:36,095 - vgg11_training - INFO - batch:14/160, loss:0.1192, batch_time:0.0863\n\n2024-03-28 09:18:39,749 - vgg11_training - INFO - batch:15/160, loss:0.0498, batch_time:0.0868\n\n2024-03-28 09:18:43,219 - vgg11_training - INFO - batch:16/160, loss:0.0548, batch_time:0.0860\n\n2024-03-28 09:18:46,750 - vgg11_training - INFO - batch:17/160, loss:0.0219, batch_time:0.0864\n\n2024-03-28 09:18:50,398 - vgg11_training - INFO - batch:18/160, loss:0.0150, batch_time:0.0925\n\n2024-03-28 09:18:53,576 - vgg11_training - INFO - batch:19/160, loss:0.0087, batch_time:0.0866\n\n2024-03-28 09:18:57,083 - vgg11_training - INFO - batch:20/160, loss:0.1220, batch_time:0.0867\n\n2024-03-28 09:19:00,650 - vgg11_training - INFO - batch:21/160, loss:0.0302, batch_time:0.0866\n\n2024-03-28 09:19:04,136 - vgg11_training - INFO - batch:22/160, loss:0.1612, batch_time:0.0867\n\n2024-03-28 09:19:07,365 - vgg11_training - INFO - batch:23/160, loss:0.0474, batch_time:0.0863\n\n2024-03-28 09:19:11,169 - vgg11_training - INFO - batch:24/160, loss:0.0124, batch_time:0.0869\n\n2024-03-28 09:19:14,771 - vgg11_training - INFO - batch:25/160, loss:0.0146, batch_time:0.0863\n\n2024-03-28 09:19:18,119 - vgg11_training - INFO - batch:26/160, loss:0.0319, batch_time:0.0866\n\n2024-03-28 09:19:21,773 - vgg11_training - INFO - batch:27/160, loss:0.0308, batch_time:0.0863\n\n2024-03-28 09:19:25,185 - vgg11_training - INFO - batch:28/160, loss:0.0122, batch_time:0.0862\n\n2024-03-28 09:19:28,771 - vgg11_training - INFO - batch:29/160, loss:0.0553, batch_time:0.0862\n\n2024-03-28 09:19:32,438 - vgg11_training - INFO - batch:30/160, loss:0.0361, batch_time:0.0869\n\n2024-03-28 09:19:35,821 - vgg11_training - INFO - batch:31/160, loss:0.0814, batch_time:0.0923\n\n2024-03-28 09:19:40,152 - vgg11_training - INFO - batch:32/160, loss:0.0230, batch_time:0.0901\n\n2024-03-28 09:19:44,213 - vgg11_training - INFO - batch:33/160, loss:0.0522, batch_time:0.0871\n\n2024-03-28 09:19:47,830 - vgg11_training - INFO - batch:34/160, loss:0.0691, batch_time:0.0867\n\n2024-03-28 09:19:51,615 - vgg11_training - INFO - batch:35/160, loss:0.1062, batch_time:0.0866\n\n2024-03-28 09:19:55,402 - vgg11_training - INFO - batch:36/160, loss:0.0463, batch_time:0.0869\n\n2024-03-28 09:19:58,818 - vgg11_training - INFO - batch:37/160, loss:0.0876, batch_time:0.0873\n\n2024-03-28 09:20:02,754 - vgg11_training - INFO - batch:38/160, loss:0.0426, batch_time:0.0862\n\n2024-03-28 09:20:06,276 - vgg11_training - INFO - batch:39/160, loss:0.1391, batch_time:0.0862\n\n2024-03-28 09:20:09,903 - vgg11_training - INFO - batch:40/160, loss:0.0136, batch_time:0.0868\n\n2024-03-28 09:20:13,571 - vgg11_training - INFO - batch:41/160, loss:0.0193, batch_time:0.0867\n\n2024-03-28 09:20:17,313 - vgg11_training - INFO - batch:42/160, loss:0.0320, batch_time:0.0864\n\n2024-03-28 09:20:21,118 - vgg11_training - INFO - batch:43/160, loss:0.0076, batch_time:0.0863\n\n2024-03-28 09:20:24,890 - vgg11_training - INFO - batch:44/160, loss:0.0072, batch_time:0.0923\n\n2024-03-28 09:20:28,519 - vgg11_training - INFO - batch:45/160, loss:0.0204, batch_time:0.0864\n\n2024-03-28 09:20:32,215 - vgg11_training - INFO - batch:46/160, loss:0.0618, batch_time:0.0869\n\n2024-03-28 09:20:35,825 - vgg11_training - INFO - batch:47/160, loss:0.0928, batch_time:0.0866\n\n2024-03-28 09:20:39,644 - vgg11_training - INFO - batch:48/160, loss:0.1405, batch_time:0.0865\n\n2024-03-28 09:20:43,276 - vgg11_training - INFO - batch:49/160, loss:0.0913, batch_time:0.0860\n\n2024-03-28 09:20:46,715 - vgg11_training - INFO - batch:50/160, loss:0.1152, batch_time:0.0866\n\n2024-03-28 09:20:49,969 - vgg11_training - INFO - batch:51/160, loss:0.0113, batch_time:0.0870\n\n2024-03-28 09:20:53,468 - vgg11_training - INFO - batch:52/160, loss:0.0971, batch_time:0.0865\n\n2024-03-28 09:20:57,003 - vgg11_training - INFO - batch:53/160, loss:0.1414, batch_time:0.0860\n\n2024-03-28 09:21:00,743 - vgg11_training - INFO - batch:54/160, loss:0.1078, batch_time:0.0865\n\n2024-03-28 09:21:04,132 - vgg11_training - INFO - batch:55/160, loss:0.0497, batch_time:0.0864\n\n2024-03-28 09:21:07,911 - vgg11_training - INFO - batch:56/160, loss:0.1133, batch_time:0.0863\n\n2024-03-28 09:21:11,733 - vgg11_training - INFO - batch:57/160, loss:0.0510, batch_time:0.0925\n\n2024-03-28 09:21:15,744 - vgg11_training - INFO - batch:58/160, loss:0.0309, batch_time:0.0865\n\n2024-03-28 09:21:19,715 - vgg11_training - INFO - batch:59/160, loss:0.0727, batch_time:0.0870\n\n2024-03-28 09:21:23,427 - vgg11_training - INFO - batch:60/160, loss:0.0609, batch_time:0.0861\n\n2024-03-28 09:21:26,775 - vgg11_training - INFO - batch:61/160, loss:0.0470, batch_time:0.0864\n\n2024-03-28 09:21:30,700 - vgg11_training - INFO - batch:62/160, loss:0.1176, batch_time:0.0866\n\n2024-03-28 09:21:34,194 - vgg11_training - INFO - batch:63/160, loss:0.0851, batch_time:0.0864\n\n2024-03-28 09:21:37,865 - vgg11_training - INFO - batch:64/160, loss:0.0260, batch_time:0.0866\n\n2024-03-28 09:21:41,534 - vgg11_training - INFO - batch:65/160, loss:0.0543, batch_time:0.0864\n\n2024-03-28 09:21:44,874 - vgg11_training - INFO - batch:66/160, loss:0.0576, batch_time:0.0867\n\n2024-03-28 09:21:48,418 - vgg11_training - INFO - batch:67/160, loss:0.1063, batch_time:0.0866\n\n2024-03-28 09:21:51,623 - vgg11_training - INFO - batch:68/160, loss:0.0906, batch_time:0.0863\n\n2024-03-28 09:21:55,032 - vgg11_training - INFO - batch:69/160, loss:0.0255, batch_time:0.0862\n\n2024-03-28 09:21:58,547 - vgg11_training - INFO - batch:70/160, loss:0.0375, batch_time:0.0927\n\n2024-03-28 09:22:01,931 - vgg11_training - INFO - batch:71/160, loss:0.1354, batch_time:0.0864\n\n2024-03-28 09:22:05,465 - vgg11_training - INFO - batch:72/160, loss:0.0672, batch_time:0.0861\n\n2024-03-28 09:22:09,080 - vgg11_training - INFO - batch:73/160, loss:0.0476, batch_time:0.0880\n\n2024-03-28 09:22:12,723 - vgg11_training - INFO - batch:74/160, loss:0.0945, batch_time:0.0862\n\n2024-03-28 09:22:16,077 - vgg11_training - INFO - batch:75/160, loss:0.0218, batch_time:0.0867\n\n2024-03-28 09:22:19,724 - vgg11_training - INFO - batch:76/160, loss:0.0872, batch_time:0.0866\n\n2024-03-28 09:22:23,539 - vgg11_training - INFO - batch:77/160, loss:0.0271, batch_time:0.0865\n\n2024-03-28 09:22:26,787 - vgg11_training - INFO - batch:78/160, loss:0.0567, batch_time:0.0863\n\n2024-03-28 09:22:30,321 - vgg11_training - INFO - batch:79/160, loss:0.0554, batch_time:0.0862\n\n2024-03-28 09:22:33,964 - vgg11_training - INFO - batch:80/160, loss:0.0751, batch_time:0.0862\n\n2024-03-28 09:22:37,288 - vgg11_training - INFO - batch:81/160, loss:0.1485, batch_time:0.0860\n\n2024-03-28 09:22:40,781 - vgg11_training - INFO - batch:82/160, loss:0.0745, batch_time:0.0864\n\n2024-03-28 09:22:44,355 - vgg11_training - INFO - batch:83/160, loss:0.1187, batch_time:0.0925\n\n2024-03-28 09:22:47,911 - vgg11_training - INFO - batch:84/160, loss:0.0671, batch_time:0.0864\n\n2024-03-28 09:22:51,476 - vgg11_training - INFO - batch:85/160, loss:0.1461, batch_time:0.0865\n\n2024-03-28 09:22:55,087 - vgg11_training - INFO - batch:86/160, loss:0.0202, batch_time:0.0861\n\n2024-03-28 09:22:58,728 - vgg11_training - INFO - batch:87/160, loss:0.0168, batch_time:0.0878\n\n2024-03-28 09:23:02,345 - vgg11_training - INFO - batch:88/160, loss:0.0217, batch_time:0.0862\n\n2024-03-28 09:23:05,790 - vgg11_training - INFO - batch:89/160, loss:0.0490, batch_time:0.0868\n\n2024-03-28 09:23:09,389 - vgg11_training - INFO - batch:90/160, loss:0.0914, batch_time:0.0867\n\n2024-03-28 09:23:13,149 - vgg11_training - INFO - batch:91/160, loss:0.0175, batch_time:0.0863\n\n2024-03-28 09:23:16,753 - vgg11_training - INFO - batch:92/160, loss:0.0592, batch_time:0.0866\n\n2024-03-28 09:23:20,399 - vgg11_training - INFO - batch:93/160, loss:0.0100, batch_time:0.0866\n\n2024-03-28 09:23:23,957 - vgg11_training - INFO - batch:94/160, loss:0.0493, batch_time:0.0863\n\n2024-03-28 09:23:27,417 - vgg11_training - INFO - batch:95/160, loss:0.1274, batch_time:0.0867\n\n2024-03-28 09:23:31,099 - vgg11_training - INFO - batch:96/160, loss:0.0957, batch_time:0.0921\n\n2024-03-28 09:23:34,683 - vgg11_training - INFO - batch:97/160, loss:0.1126, batch_time:0.0862\n\n2024-03-28 09:23:38,322 - vgg11_training - INFO - batch:98/160, loss:0.0343, batch_time:0.0862\n\n2024-03-28 09:23:42,037 - vgg11_training - INFO - batch:99/160, loss:0.0523, batch_time:0.0867\n\n2024-03-28 09:23:45,583 - vgg11_training - INFO - batch:100/160, loss:0.0245, batch_time:0.0864\n\n2024-03-28 09:23:49,262 - vgg11_training - INFO - batch:101/160, loss:0.0325, batch_time:0.0863\n\n2024-03-28 09:23:52,760 - vgg11_training - INFO - batch:102/160, loss:0.0306, batch_time:0.0865\n\n2024-03-28 09:23:56,620 - vgg11_training - INFO - batch:103/160, loss:0.0185, batch_time:0.0867\n\n2024-03-28 09:24:00,220 - vgg11_training - INFO - batch:104/160, loss:0.0728, batch_time:0.0864\n\n2024-03-28 09:24:04,059 - vgg11_training - INFO - batch:105/160, loss:0.1265, batch_time:0.0866\n\n2024-03-28 09:24:07,501 - vgg11_training - INFO - batch:106/160, loss:0.0207, batch_time:0.0869\n\n2024-03-28 09:24:11,252 - vgg11_training - INFO - batch:107/160, loss:0.0224, batch_time:0.0869\n\n2024-03-28 09:24:15,021 - vgg11_training - INFO - batch:108/160, loss:0.1080, batch_time:0.0864\n\n2024-03-28 09:24:18,905 - vgg11_training - INFO - batch:109/160, loss:0.0255, batch_time:0.0926\n\n2024-03-28 09:24:22,562 - vgg11_training - INFO - batch:110/160, loss:0.1374, batch_time:0.0864\n\n2024-03-28 09:24:26,169 - vgg11_training - INFO - batch:111/160, loss:0.0475, batch_time:0.0893\n\n2024-03-28 09:24:29,914 - vgg11_training - INFO - batch:112/160, loss:0.0169, batch_time:0.0865\n\n2024-03-28 09:24:33,686 - vgg11_training - INFO - batch:113/160, loss:0.1270, batch_time:0.0873\n\n2024-03-28 09:24:37,165 - vgg11_training - INFO - batch:114/160, loss:0.0278, batch_time:0.0864\n\n2024-03-28 09:24:40,733 - vgg11_training - INFO - batch:115/160, loss:0.0389, batch_time:0.0861\n\n2024-03-28 09:24:44,512 - vgg11_training - INFO - batch:116/160, loss:0.0109, batch_time:0.0863\n\n2024-03-28 09:24:47,967 - vgg11_training - INFO - batch:117/160, loss:0.0148, batch_time:0.0862\n\n2024-03-28 09:24:51,650 - vgg11_training - INFO - batch:118/160, loss:0.0172, batch_time:0.0864\n\n2024-03-28 09:24:55,225 - vgg11_training - INFO - batch:119/160, loss:0.0916, batch_time:0.0867\n\n2024-03-28 09:24:59,200 - vgg11_training - INFO - batch:120/160, loss:0.1055, batch_time:0.0866\n\n2024-03-28 09:25:02,833 - vgg11_training - INFO - batch:121/160, loss:0.0457, batch_time:0.0867\n\n2024-03-28 09:25:06,347 - vgg11_training - INFO - batch:122/160, loss:0.0317, batch_time:0.0937\n\n2024-03-28 09:25:10,007 - vgg11_training - INFO - batch:123/160, loss:0.0528, batch_time:0.0865\n\n2024-03-28 09:25:13,756 - vgg11_training - INFO - batch:124/160, loss:0.1383, batch_time:0.0863\n\n2024-03-28 09:25:17,175 - vgg11_training - INFO - batch:125/160, loss:0.0946, batch_time:0.0864\n\n2024-03-28 09:25:20,818 - vgg11_training - INFO - batch:126/160, loss:0.0886, batch_time:0.0868\n\n2024-03-28 09:25:24,317 - vgg11_training - INFO - batch:127/160, loss:0.0625, batch_time:0.0862\n\n2024-03-28 09:25:27,892 - vgg11_training - INFO - batch:128/160, loss:0.1300, batch_time:0.0862\n\n2024-03-28 09:25:31,500 - vgg11_training - INFO - batch:129/160, loss:0.0857, batch_time:0.0861\n\n2024-03-28 09:25:35,280 - vgg11_training - INFO - batch:130/160, loss:0.0279, batch_time:0.0864\n\n2024-03-28 09:25:38,740 - vgg11_training - INFO - batch:131/160, loss:0.0161, batch_time:0.0860\n\n2024-03-28 09:25:42,196 - vgg11_training - INFO - batch:132/160, loss:0.0695, batch_time:0.0860\n\n2024-03-28 09:25:45,867 - vgg11_training - INFO - batch:133/160, loss:0.0594, batch_time:0.0866\n\n2024-03-28 09:25:49,537 - vgg11_training - INFO - batch:134/160, loss:0.1778, batch_time:0.0866\n\n2024-03-28 09:25:53,432 - vgg11_training - INFO - batch:135/160, loss:0.0308, batch_time:0.0928\n\n2024-03-28 09:25:56,977 - vgg11_training - INFO - batch:136/160, loss:0.0163, batch_time:0.0863\n\n2024-03-28 09:26:00,706 - vgg11_training - INFO - batch:137/160, loss:0.0765, batch_time:0.0863\n\n2024-03-28 09:26:04,337 - vgg11_training - INFO - batch:138/160, loss:0.0812, batch_time:0.0862\n\n2024-03-28 09:26:07,962 - vgg11_training - INFO - batch:139/160, loss:0.0450, batch_time:0.0860\n\n2024-03-28 09:26:11,457 - vgg11_training - INFO - batch:140/160, loss:0.0310, batch_time:0.0864\n\n2024-03-28 09:26:15,246 - vgg11_training - INFO - batch:141/160, loss:0.0104, batch_time:0.0870\n\n2024-03-28 09:26:18,837 - vgg11_training - INFO - batch:142/160, loss:0.1135, batch_time:0.0864\n\n2024-03-28 09:26:22,595 - vgg11_training - INFO - batch:143/160, loss:0.0417, batch_time:0.0866\n\n2024-03-28 09:26:26,651 - vgg11_training - INFO - batch:144/160, loss:0.0484, batch_time:0.0867\n\n2024-03-28 09:26:30,502 - vgg11_training - INFO - batch:145/160, loss:0.0270, batch_time:0.0867\n\n2024-03-28 09:26:33,956 - vgg11_training - INFO - batch:146/160, loss:0.0345, batch_time:0.0865\n\n2024-03-28 09:26:37,557 - vgg11_training - INFO - batch:147/160, loss:0.0245, batch_time:0.0870\n\n2024-03-28 09:26:41,127 - vgg11_training - INFO - batch:148/160, loss:0.0633, batch_time:0.0920\n\n2024-03-28 09:26:44,788 - vgg11_training - INFO - batch:149/160, loss:0.0129, batch_time:0.0864\n\n2024-03-28 09:26:48,702 - vgg11_training - INFO - batch:150/160, loss:0.0155, batch_time:0.0861\n\n2024-03-28 09:26:52,370 - vgg11_training - INFO - batch:151/160, loss:0.0432, batch_time:0.0865\n\n2024-03-28 09:26:55,930 - vgg11_training - INFO - batch:152/160, loss:0.0500, batch_time:0.0862\n\n2024-03-28 09:26:59,330 - vgg11_training - INFO - batch:153/160, loss:0.0160, batch_time:0.0868\n\n2024-03-28 09:27:02,645 - vgg11_training - INFO - batch:154/160, loss:0.0058, batch_time:0.0862\n\n2024-03-28 09:27:06,333 - vgg11_training - INFO - batch:155/160, loss:0.0582, batch_time:0.0859\n\n2024-03-28 09:27:10,064 - vgg11_training - INFO - batch:156/160, loss:0.0551, batch_time:0.0862\n\n2024-03-28 09:27:13,588 - vgg11_training - INFO - batch:157/160, loss:0.0200, batch_time:0.0863\n\n2024-03-28 09:27:17,413 - vgg11_training - INFO - batch:158/160, loss:0.0543, batch_time:0.0870\n\n2024-03-28 09:27:20,986 - vgg11_training - INFO - batch:159/160, loss:0.0209, batch_time:0.0861\n\n2024-03-28 09:27:23,618 - vgg11_training - INFO - batch:160/160, loss:0.0563, batch_time:0.0782\n\n2024-03-28 09:27:23,620 - vgg11_training - INFO - Epoch 3/20, total_loss: 9.3044\n\n2024-03-28 09:27:24,582 - vgg11_training - INFO - Test_epoch:3/20\n\n2024-03-28 09:29:39,828 - vgg11_training - INFO - Test Accuracy: 0.9847, Test Precision: 0.9822, Test Recall: 0.9863, Test F1: 0.9841 Test time: 135.2447633743286\n\n2024-03-28 09:29:39,829 - vgg11_training - INFO - epoch:3/20, total_time:0:11:53\n\n2024-03-28 09:29:39,832 - vgg11_training - INFO - Train_epoch:4/20\n\n2024-03-28 09:29:43,438 - vgg11_training - INFO - batch:1/160, loss:0.0121, batch_time:0.0871\n\n2024-03-28 09:29:46,784 - vgg11_training - INFO - batch:2/160, loss:0.0199, batch_time:0.0923\n\n2024-03-28 09:29:50,556 - vgg11_training - INFO - batch:3/160, loss:0.0707, batch_time:0.0864\n\n2024-03-28 09:29:54,247 - vgg11_training - INFO - batch:4/160, loss:0.0452, batch_time:0.0864\n\n2024-03-28 09:29:57,644 - vgg11_training - INFO - batch:5/160, loss:0.0092, batch_time:0.0873\n\n2024-03-28 09:30:00,872 - vgg11_training - INFO - batch:6/160, loss:0.0124, batch_time:0.0868\n\n2024-03-28 09:30:04,630 - vgg11_training - INFO - batch:7/160, loss:0.0218, batch_time:0.0865\n\n2024-03-28 09:30:07,934 - vgg11_training - INFO - batch:8/160, loss:0.0348, batch_time:0.0866\n\n2024-03-28 09:30:11,846 - vgg11_training - INFO - batch:9/160, loss:0.0388, batch_time:0.0865\n\n2024-03-28 09:30:15,673 - vgg11_training - INFO - batch:10/160, loss:0.0645, batch_time:0.0864\n\n2024-03-28 09:30:19,413 - vgg11_training - INFO - batch:11/160, loss:0.0172, batch_time:0.0870\n\n2024-03-28 09:30:23,361 - vgg11_training - INFO - batch:12/160, loss:0.0693, batch_time:0.0866\n\n2024-03-28 09:30:26,896 - vgg11_training - INFO - batch:13/160, loss:0.2092, batch_time:0.0870\n\n2024-03-28 09:30:30,432 - vgg11_training - INFO - batch:14/160, loss:0.2002, batch_time:0.0865\n\n2024-03-28 09:30:34,014 - vgg11_training - INFO - batch:15/160, loss:0.0217, batch_time:0.0921\n\n2024-03-28 09:30:37,834 - vgg11_training - INFO - batch:16/160, loss:0.0319, batch_time:0.0864\n\n2024-03-28 09:30:40,940 - vgg11_training - INFO - batch:17/160, loss:0.0238, batch_time:0.0861\n\n2024-03-28 09:30:44,668 - vgg11_training - INFO - batch:18/160, loss:0.1209, batch_time:0.0869\n\n2024-03-28 09:30:48,348 - vgg11_training - INFO - batch:19/160, loss:0.1165, batch_time:0.0869\n\n2024-03-28 09:30:51,997 - vgg11_training - INFO - batch:20/160, loss:0.0154, batch_time:0.0866\n\n2024-03-28 09:30:55,692 - vgg11_training - INFO - batch:21/160, loss:0.1028, batch_time:0.0865\n\n2024-03-28 09:30:59,670 - vgg11_training - INFO - batch:22/160, loss:0.0607, batch_time:0.0863\n\n2024-03-28 09:31:03,426 - vgg11_training - INFO - batch:23/160, loss:0.0239, batch_time:0.0876\n\n2024-03-28 09:31:07,019 - vgg11_training - INFO - batch:24/160, loss:0.0289, batch_time:0.0870\n\n2024-03-28 09:31:10,685 - vgg11_training - INFO - batch:25/160, loss:0.0295, batch_time:0.0869\n\n2024-03-28 09:31:14,497 - vgg11_training - INFO - batch:26/160, loss:0.0626, batch_time:0.0865\n\n2024-03-28 09:31:18,144 - vgg11_training - INFO - batch:27/160, loss:0.0460, batch_time:0.0865\n\n","output_type":"stream"}]}]}